{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad74adf3",
   "metadata": {
    "id": "ad74adf3"
   },
   "source": [
    "# Ekstrakcja Źródeł\n",
    "\n",
    "<img src=\"https://live.staticflickr.com/65535/54443002259_4a8e1249dd_b.jpg\" alt=\"Embedded Photo\" width=\"500\">\n",
    "\n",
    "*Obraz wygenerowany za pomocą ChatGPT.*\n",
    "\n",
    "## Wstęp\n",
    "\n",
    "Modele językowe bywają skłonne do mówienia nieprawdy lub półprawdy, a także zmyślania faktów bez podawania źródeł. Obecnie coraz częściej używane są systemy, które zamiast odpowiadać na pytania bezpośrednio, wpierw przeszukują bazę danych, np. zbiór dokumentów, i dopiero na podstawie najlepiej pasujących dokumentów generują odpowiedź. Taka odpowiedź ma większe szanse być oparta na rzeczywistości i może być zweryfikowana przez człowieka -- o ile poprawnie odnalezione zostały właściwe źródła.\n",
    "\n",
    "Oczywiście, źródeł może być bardzo dużo, więc metody przeszukiwania muszą być efektywne -- przetworzenie wszystkiego \"na raz\" bezpośrednio modelem językowym nie wchodzi w grę! W tym zadaniu skupisz się na znajdowaniu najlepszych źródeł dla zadanego zdania, korzystając z metody **embeddingów** (pl. zanurzeń wektorów).\n",
    "\n",
    "Wyobraź sobie, że jesteś inżynierem AI w firmie opracowującej narzędzie do weryfikacji faktów naukowych. Twoim zadaniem jest stworzenie modułu, który potrafi szybko i skutecznie odnajdywać wiarygodne publikacje naukowe potwierdzające lub obalające konkretne stwierdzenia. Dzięki Twojemu rozwiązaniu, naukowcy, dziennikarze i decydenci będą mogli weryfikować informacje w oparciu o solidne podstawy naukowe, co jest szczególnie istotne w dobie dezinformacji.\n",
    "\n",
    "\n",
    "## Zadanie\n",
    "\n",
    "Twoim zadaniem jest opracowanie systemu, który generuje wysokiej jakości wektorowe reprezentacje (embeddingi) zarówno dla zapytań, jak i dokumentów źródłowych, umożliwiające precyzyjne dopasowanie właściwych źródeł do zapytań.\n",
    "\n",
    "Mając do dyspozycji **queries** (pl. zbiór zapytań; zapytania na które szukamy źródeł) oraz **corpus** (pl. baza dokumentów/źródeł; zbiór rozważanych dokumentów), musisz zaimplementować funkcje, które przypisują zapytaniom oraz źródłom wektory liczb rzeczywistych o wymiarze $768$. Te wektory będą użyte do znalezienia źródeł dla każdego zapytania przez dostarczoną przez nas funkcję ewaluacyjną, która dla danego zapytania, ze zbioru dokumentów wybiera $k=10$ najbliższych sąsiadów (ang. $k$-Nearest Neighbours).\n",
    "\n",
    "W rozwiązaniu możesz skorzystać z dostarczonego modelu bazującego na architekturze GPT2, który został specjalnie dotrenowany, aby był pomocny w otrzymywaniu dobrej jakości embeddingów.\n",
    "\n",
    "W trakcie pracy nad rozwiązaniem będziesz mógł testować jego skuteczność na zbiorze walidacyjnym, który pozwoli Ci ocenić jakość generowanych embeddingów w kontekście zadania wyszukiwania właściwych dokumentów źródłowych.\n",
    "\n",
    "### Dane\n",
    "\n",
    "Dostępne dla Ciebie w tym zadaniu dane to:\n",
    "\n",
    "- Zbiór zapytań (queries), dla których należy znaleźć odpowiednie źródła\n",
    "- Korpus dokumentów (corpus), zawierający publikacje naukowe, które mogą być źródłami dla zapytań\n",
    "- Informacje o dopasowaniu zapytań do dokumentów w zbiorze walidacyjnym\n",
    "\n",
    "Twoje rozwiązanie będzie oceniane na benchmarku *SciFact*. Służy on do oceny systemów wyszukiwania i weryfikacji faktów w kontekście naukowym. Składa się z zestawu stwierdzeń (ang. queries) opartych na rzeczywistych publikacjach naukowych, a baza dokumentów (ang. corpus) to publikacje z zakresu nauk przyrodniczych i medycznych. Do każdego stwierdzenia istnieje co najmniej jedna publikacja, która je popiera lub obala. Dostarczamy kod służący do ładowania danych, więc dane opisujemy tu wyłącznie informacyjnie.\n",
    "\n",
    "\n",
    "**Plik `corpus.jsonl`** zawiera unikalne identyfikatory, tytuły i streszczenia prac naukowych\n",
    "\n",
    "Przykład pojedynczego dokumentu:\n",
    "```\n",
    "{\n",
    "    \"text_id\": 13734012,\n",
    "    \"title\": \"Prevalent abnormal prion protein in human appendixes after bovine spongiform encephalopathy epizootic: large scale survey\",\n",
    "    \"text\": \"OBJECTIVES To carry out a further survey (...) CONCLUSIONS This study corroborates previous studies and suggests a high prevalence of infection with abnormal PrP, indicating vCJD carrier status in the population compared with the 177 vCJD cases to date. These findings have important implications for the management of blood and blood products and for the handling of surgical instruments.\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Plik `queries_val.jsonl`** zawiera treści stwierdzeń oraz identyfikator pasującego tekstu źródłowego. Zbiór testowy, na których finalnie będzie oceniane Twoje rozwiązanie **nie będzie zawierał** identyfikatorów pasujących tekstów źródłowych.\n",
    "\n",
    "Przykład pojedynczego zapytania:\n",
    "```\n",
    "{\n",
    "    \"query\": \"1 in 5 million in UK have abnormal PrP positivity.\",\n",
    "    \"matching_text_id\": 13734012\n",
    "}\n",
    "```\n",
    "\n",
    "### Kryterium Oceny\n",
    "Zaimplementowane przez Ciebie metody (funkcje) `Embedder.encode_queries` oraz `Embedder.encode_corpus` zostaną wykorzystane aby przetworzyć odpowiednio zapytania $q \\in Q$ a także dokumenty $d \\in C$ na wektory. W dalszej części będziemy wymiennie używać $q$ i $d$ zarówno w kontekście tekstów jak i ich embeddingów.\n",
    "\n",
    "Załóżmy, że zapytaniu $q\\in Q$ odpowiada złoty dokument $d\\in C$.\n",
    "Kod ewaluacyjny sortuje wszystkie dokumenty według odległości od $q$, otrzymując dokumenty $K_1, K_2, ..., K_n$, tak że $K_1$ jest najbliżej. Następnie oznaczamy jako $I$, indeks złotego dokumentu $d$ w tym ciągu. To znaczy, że $I - 1$ jest liczbą dokumentów, których odległość od $q$ jest mniejsza, niż odległość $q$ od $d$.\n",
    "\n",
    "Odległość między wektorami liczymy za pomocą podobieństwa cosinusowego (ang. cosine similarity), które dla wektorów $v, w \\in \\mathbb{R}^n$ jest określone jako $\\frac{v^Tw}{||v|| \\cdot ||w||}$, gdzie $||v||$ to długość wektora $v$.\n",
    "\n",
    "Wynik dla zapytania $q$ określamy jako  \n",
    "\n",
    "$$\\text{nDCG@10}(q) = \\begin{cases}\n",
    "\\log_2(I + 1) & \\text{jeśli $I \\leq 10$} \\\\\n",
    "0 & \\text{w przeciwnym wypadku.}\n",
    "\\end{cases}$$\n",
    "\n",
    "Czyli, im bliżej złoty dokument został umieszczony zapytania względem innych dokumentów, tym wyższy wynik -- jeśli 10 \"złych\" dokumentów jest bliżej zapytania to wynik za ten przykład to 0.\n",
    "\n",
    "Ostatecznie ocena Twojego rozwiązania będzie opierać się na metryce **nDCG@10**, obliczanej jako średnia wartość tej metryki dla wszystkich zapytań $(q \\in Q )$.\n",
    "\n",
    "- Jeśli wynik **nDCG@10** będzie **niższy niż 0.2**, otrzymasz **0 punktów**.  \n",
    "- Jeśli wynik **przekroczy 0.5**, otrzymasz **maksymalną liczbę punktów**, czyli **100**.  \n",
    "\n",
    "Punktacja dla wartości pomiędzy tymi progami będzie naliczana proporcjonalnie.\n",
    "\n",
    "\n",
    "## Ograniczenia\n",
    "\n",
    "- Twoje rozwiazanie będzie testowane na Platformie Konkursowej bez dostępu do internetu oraz w środowisku z GPU.\n",
    "- Ewaluacja Twojego finalnego rozwiązania na Platformie Konkursowej nie może trwać dłużej niż 10 minut z GPU.\n",
    "- Embedding każdego zapytania oraz tekstu powinien mieć wymiar 768\n",
    "- Lista dopuszczalnych bibliotek: `torch`, `pandas`, `numpy`, `nltk`, `transformers`.\n",
    "\n",
    "## Pliki Zgłoszeniowe\n",
    "\n",
    "Należy przesłać tylko ten notebook uzupełniony o Twoje rozwiązanie (patrz klasa `Embedder`).\n",
    "\n",
    "## Wskazówki\n",
    "\n",
    "- Model GPT2 jest modelem językowym typu dekoder. Modele typu dekoder działają tak, że dla danego ciągu tokenów (np. prefiksu przetwarzanego zdania) $t_1, t_2, \\dots, t_n$ wyliczają ukryty wektor $h_{n+1} \\in \\mathbb{R}^d$, a następnie transformują go jedną ze swoich macierzy z wagami na $p_{n+1} \\in \\mathbb{R}^m$ -- rozkład prawdopodobieństwa na tokenach w słowniku.\n",
    "- W porównaniu z dostępnym czasem wykonania, dokumentów jest wiele.\n",
    "\n",
    "## Ewaluacja\n",
    "\n",
    "Podczas sprawdzania flaga `FINAL_EVALUATION_MODE` zostanie ustawiona na `True`.\n",
    "\n",
    "Za to zadanie możesz zdobyć pomiędzy 0 a 100 punktów. Liczba punktów, którą zdobędziesz, będzie wyliczona na (tajnym) zbiorze testowym na Platformie Konkursowej na podstawie wyżej wspomnianego wzoru, zaokrąglona do liczby całkowitej. Jeśli Twoje rozwiązanie nie będzie spełniało powyższych kryteriów lub nie będzie wykonywać się prawidłowo, otrzymasz za zadanie 0 punktów."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01485926",
   "metadata": {
    "id": "01485926"
   },
   "source": [
    "# Kod Startowy\n",
    "\n",
    "W tej sekcji inicjalizujemy środowisko poprzez zaimportowanie potrzebnych bibliotek i funkcji. Przygotowany kod tokenizatora, ładowania danych i ewaluacji ulatwi Ci operowanie na danych i pozwoli rozwiązać zadanie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e81e8ad",
   "metadata": {
    "id": "1e81e8ad"
   },
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "FINAL_EVALUATION_MODE = False  # W czasie sprawdzania Twojego rozwiązania, zmienimy tą wartość na True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5a25afc",
   "metadata": {
    "id": "a5a25afc"
   },
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "import json\n",
    "import os\n",
    "from math import log2\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, tokenizer_path, length=150):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.tokenizer.padding_side = \"right\"\n",
    "        self.length = length\n",
    "\n",
    "    def __call__(self, batch_text):\n",
    "        batch_tensor = self.tokenizer(\n",
    "            batch_text,\n",
    "            max_length=self.length,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return batch_tensor.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff09408c",
   "metadata": {
    "id": "ff09408c"
   },
   "source": [
    "## Ładowanie Danych\n",
    "W tej części zadania załadujemy dane treningowe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebb43789",
   "metadata": {
    "id": "ebb43789",
    "outputId": "4cfae7e8-0e4c-40ea-c6ff-6fdbde901b0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5183 texts and 300 queries.\n"
     ]
    }
   ],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "def load_corpus(file):\n",
    "    corpus = {}\n",
    "    with open(file, encoding=\"utf8\") as f_in:\n",
    "        for line in f_in:\n",
    "            line = json.loads(line)\n",
    "            corpus[line.get(\"text_id\")] = {\n",
    "                \"text\": line.get(\"text\"),\n",
    "                \"title\": line.get(\"title\"),\n",
    "            }\n",
    "    return corpus\n",
    "\n",
    "def load_queries(file):\n",
    "    queries = {}\n",
    "    matching_texts = {}\n",
    "    with open(file, encoding=\"utf8\") as f_in:\n",
    "        for query_num, line in enumerate(f_in):\n",
    "            line = json.loads(line)\n",
    "\n",
    "            queries[query_num] = line.get(\"query\")\n",
    "            matching_texts[query_num] = line.get(\"matching_text_id\")\n",
    "    return queries, matching_texts\n",
    "\n",
    "corpus = load_corpus(\"corpus.jsonl\")\n",
    "queries, matching_texts = load_queries(\"queries_val.jsonl\")\n",
    "\n",
    "print(f\"Loaded {len(corpus)} texts and {len(queries)} queries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be621dd2",
   "metadata": {
    "id": "be621dd2"
   },
   "source": [
    "## Kod z Kryterium Oceniającym\n",
    "\n",
    "Kod, zbliżony do poniższego, będzie używany do oceny rozwiązania na zbiorze testowym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7807df2a",
   "metadata": {
    "id": "7807df2a"
   },
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "def evaluate_retrieval_ndcg(\n",
    "    golden_matches: dict[int, int],\n",
    "    results: dict[int, dict[int, float]],\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Oblicza wartość metryki nDCG dla podanych wyników wyszukiwania.\n",
    "\n",
    "    Funkcja oblicza wynik Twojego rozwiązania, bazując na wynikach dla $top\\\\_k$ najlepszych dokumentów według Twojego embeddera.\n",
    "\n",
    "    :param golden_matches: Słownik ze złotymi przyporządkowaniami, gdzie kluczem jest id zapytania, a wartością jest id właściwego dokumentu.\n",
    "    :param results: Słownik z wynikami wyszukiwania, gdzie kluczem jest id zapytania, a wartością jest słownik z id dokumentów i ich podobieństwami do danego zapytania.\n",
    "    :return: Wartość metryki nDCG.\"\"\"\n",
    "\n",
    "    for query_id, v in results.items():\n",
    "        results[query_id] = {k: v for k, v in sorted(v.items(), key=lambda item: -item[1])}\n",
    "\n",
    "    ndcg_sum = 0\n",
    "    for query_id, v in results.items():\n",
    "        golden_document = golden_matches[query_id]\n",
    "        for i, document_id in enumerate(v.keys()):\n",
    "            if golden_document == document_id:\n",
    "                ndcg_sum += 1 / log2(i + 2)\n",
    "\n",
    "    ndcg = round(ndcg_sum / len(results), 5)\n",
    "    return ndcg\n",
    "\n",
    "\n",
    "def compute_score(ndcg: float) -> float:\n",
    "    \"\"\"\n",
    "    Oblicza wynik punktowy na podstawie wartości metryki nDCG.\n",
    "    \"\"\"\n",
    "    lower_bound = 0.2\n",
    "    upper_bound = 0.5\n",
    "\n",
    "    if ndcg <= lower_bound:\n",
    "        return 0\n",
    "    elif lower_bound < ndcg < upper_bound:\n",
    "        return int(round(100 * (ndcg - lower_bound) / (upper_bound - lower_bound)))\n",
    "    else:\n",
    "        return 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac37bf0",
   "metadata": {
    "id": "dac37bf0"
   },
   "source": [
    "### Przeszukiwanie\n",
    "Poniżej jest kod, który służy do wybierania dla danego zapytania $top\\_k$ najlepszych dokumentów z korpusu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bd9149c",
   "metadata": {
    "id": "6bd9149c"
   },
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "def cos_sim(a: torch.Tensor, b: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\n",
    "    :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\n",
    "    \"\"\"\n",
    "    a_norm = torch.nn.functional.normalize(a, p=2, dim=1)\n",
    "    b_norm = torch.nn.functional.normalize(b, p=2, dim=1)\n",
    "    return torch.mm(a_norm, b_norm.transpose(0, 1))\n",
    "\n",
    "def search_topk_texts(\n",
    "    embedder,\n",
    "    corpus: dict[str, dict[str, str]],\n",
    "    queries: dict[str, str],\n",
    "    top_k: int = 10,\n",
    ") -> dict[str, dict[str, float]]:\n",
    "    results = {}\n",
    "\n",
    "    # Create embeddings for all queries using model.encode_queries()\n",
    "    # Runs semantic search against the corpus embeddings\n",
    "    # Returns a ranked list with the corpus ids\n",
    "    query_ids = list(queries.keys())\n",
    "    results = {qid: {} for qid in query_ids}\n",
    "    queries = [queries[qid] for qid in queries]\n",
    "    query_embeddings = embedder.encode_queries(queries)\n",
    "\n",
    "    corpus_ids = sorted(\n",
    "        corpus,\n",
    "        key=lambda k: len(corpus[k].get(\"title\", \"\") + corpus[k].get(\"text\", \"\")),\n",
    "        reverse=True,\n",
    "    )\n",
    "    corpus = [corpus[cid] for cid in corpus_ids]\n",
    "\n",
    "    # Encode chunk of corpus\n",
    "    corpus_embeddings = embedder.encode_corpus(corpus)\n",
    "\n",
    "    # Compute similarites using cosine-similarity\n",
    "    cos_scores = cos_sim(query_embeddings, corpus_embeddings)\n",
    "    cos_scores[torch.isnan(cos_scores)] = -1\n",
    "\n",
    "    # Get top-k values\n",
    "    cos_scores_top_k_values, cos_scores_top_k_idx = torch.topk(\n",
    "        cos_scores,\n",
    "        min(top_k + 1, len(cos_scores[1])),\n",
    "        dim=1,\n",
    "        largest=True,\n",
    "        sorted=False,\n",
    "    )\n",
    "    cos_scores_top_k_values = cos_scores_top_k_values.cpu().tolist()\n",
    "    cos_scores_top_k_idx = cos_scores_top_k_idx.cpu().tolist()\n",
    "\n",
    "    for query_itr in range(len(query_embeddings)):\n",
    "        query_id = query_ids[query_itr]\n",
    "        for score, corpus_id in zip(cos_scores_top_k_values[query_itr], cos_scores_top_k_idx[query_itr]):\n",
    "            results[query_id][corpus_ids[corpus_id]] = score\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d80387",
   "metadata": {
    "id": "d0d80387"
   },
   "source": [
    "# Twoje Rozwiązanie\n",
    "W tej sekcji należy umieścić Twoje rozwiązanie. Wprowadzaj zmiany wyłącznie tutaj!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16ae4978",
   "metadata": {
    "id": "16ae4978"
   },
   "outputs": [],
   "source": [
    "class Embedder:\n",
    "    # Nie zmieniaj sygnatury konstruktora\n",
    "    def __init__(self):\n",
    "        # TODO: możesz zmieniać tą metodę,\n",
    "        # ale nie zmieniaj jej sygnatury! (tzn. nie zmieniaj argumentów)\n",
    "        self.model = AutoModel.from_pretrained(\"Muennighoff/SGPT-125M-weightedmean-msmarco-specb-bitfit\").to(device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"Muennighoff/SGPT-125M-weightedmean-msmarco-specb-bitfit\")\n",
    "        self.model.eval() # set model to evaluation mode\n",
    "        self.batch_size = 8  # Adjust batch size as needed. Start small and increase\n",
    "\n",
    "    def encode_queries(self, queries: list[str]):\n",
    "        \"\"\"\n",
    "        Funkcja kodująca zapytania.\n",
    "        :param queries: Lista zapytań do zakodowania\n",
    "        :return: Embeddingi zapytań - tensor o wymiarach (n, 512), gdzie n = len(queries) to liczba zapytań.\n",
    "        \"\"\"\n",
    "        # print(\"Encoding Queries\")\n",
    "        all_embeddings = []\n",
    "        for i in range(0, len(queries), self.batch_size):\n",
    "            batch_queries = queries[i:i + self.batch_size]\n",
    "\n",
    "            # Tokenize the queries\n",
    "            encoded_input = self.tokenizer(batch_queries, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "\n",
    "            # Get the model output\n",
    "            with torch.no_grad():  # Disable gradient calculation during inference\n",
    "                model_output = self.model(**encoded_input)\n",
    "\n",
    "            # Perform mean pooling to get sentence embeddings (as per SGPT paper)\n",
    "            token_embeddings = model_output[0] # First element are the hidden states\n",
    "            attention_mask = encoded_input['attention_mask']\n",
    "            input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "            # Calculate weights\n",
    "            seq_length = token_embeddings.size(1)\n",
    "            weights = torch.arange(1, seq_length + 1, dtype=torch.float, device=device)\n",
    "            weights = weights / torch.sum(weights)  # Normalize weights to sum to 1\n",
    "            weights = weights.unsqueeze(0).unsqueeze(-1).expand(token_embeddings.size()) # Expand for broadcasting\n",
    "            \n",
    "            # Weighted mean pooling\n",
    "            weighted_embeddings = token_embeddings * weights\n",
    "            sum_embeddings = torch.sum(weighted_embeddings * input_mask_expanded, 1)\n",
    "            sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "            embeddings = sum_embeddings / sum_mask\n",
    "            \n",
    "            all_embeddings.append(embeddings.cpu())\n",
    "\n",
    "            # if ((i/self.batch_size)%10)==0:\n",
    "            #     print(f\"Encoded: [{i}|{len(queries)}]\")\n",
    "            \n",
    "        return torch.cat(all_embeddings, dim=0)  # Concatenate all batch embeddings\n",
    "\n",
    "\n",
    "    def encode_corpus(self, texts: list[dict]):\n",
    "        \"\"\"\n",
    "        Funkcja kodująca teksty źródłowe.\n",
    "        :param texts: Lista tekstów do zakodowania. Każdy tekst jest reprezentowany jako słownik:\n",
    "            {\n",
    "                \"title\": \"...\"\n",
    "                \"text\": \"...\",\n",
    "            }\n",
    "        :return: Embeddingi tekstów - tensor o wymiarach (m, 512), gdzie m = len(texts) to liczba tekstów\n",
    "        \"\"\"\n",
    "        # print(\"Encoding Corupus\")\n",
    "        all_embeddings = []\n",
    "        for i in range(0, len(texts), self.batch_size):\n",
    "            batch_texts = texts[i:i + self.batch_size]\n",
    "            # Combine title and text\n",
    "            corpus_texts = [f\"{text['title']} {text['text']}\" for text in batch_texts]\n",
    "\n",
    "            # Tokenize the corpus texts\n",
    "            encoded_input = self.tokenizer(corpus_texts, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "\n",
    "            # Get the model output\n",
    "            with torch.no_grad(): # Disable gradient calculation during inference\n",
    "                model_output = self.model(**encoded_input)\n",
    "\n",
    "            # Perform mean pooling to get sentence embeddings (as per SGPT paper)\n",
    "            token_embeddings = model_output[0] # First element are the hidden states\n",
    "            attention_mask = encoded_input['attention_mask']\n",
    "            input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "            # Calculate weights\n",
    "            seq_length = token_embeddings.size(1)\n",
    "            weights = torch.arange(1, seq_length + 1, dtype=torch.float, device=device)\n",
    "            weights = weights / torch.sum(weights)  # Normalize weights to sum to 1\n",
    "            weights = weights.unsqueeze(0).unsqueeze(-1).expand(token_embeddings.size()) # Expand for broadcasting\n",
    "            \n",
    "            # Weighted mean pooling\n",
    "            weighted_embeddings = token_embeddings * weights\n",
    "            sum_embeddings = torch.sum(weighted_embeddings * input_mask_expanded, 1)\n",
    "            sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "            embeddings = sum_embeddings / sum_mask\n",
    "            \n",
    "            all_embeddings.append(embeddings.cpu())\n",
    "            # if ((i/self.batch_size)%10)==0:\n",
    "            #     print(f\"Encoded: [{i}|{len(texts)}]\")\n",
    "\n",
    "        return torch.cat(all_embeddings, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94301b8",
   "metadata": {
    "id": "f94301b8"
   },
   "source": [
    "# Ewaluacja\n",
    "\n",
    "Uruchomienie poniższej komórki pozwoli sprawdzić, ile punktów zdobyłoby Twoje rozwiązanie na danych walidacyjnych. Przed wysłaniem upewnij się, że cały notebook wykonuje się od początku do końca bez błędów i bez konieczności ingerencji użytkownika po wybraniu opcji \"Run All\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5b2c84c",
   "metadata": {
    "id": "d5b2c84c",
    "outputId": "0299285c-8e24-45cf-83d4-795714041d04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding Queries\n",
      "Encoding Corupus\n",
      "\n",
      "Liczba zapytań: 300\n",
      "Liczba tekstów: 5183\n",
      "nDCG: 0.511\n",
      "Wynik punktowy: 100\n"
     ]
    }
   ],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "if not FINAL_EVALUATION_MODE:\n",
    "    embedder = Embedder()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        results = search_topk_texts(embedder, corpus, queries, top_k=10)\n",
    "\n",
    "    # Obliczenie nDCG\n",
    "    ndcg = evaluate_retrieval_ndcg(matching_texts, results)\n",
    "\n",
    "    # Obliczenie końcowego wyniku na podstawie nDCG\n",
    "    points = compute_score(ndcg)\n",
    "\n",
    "    print(f\"\\nLiczba zapytań: {len(queries)}\")\n",
    "    print(f\"Liczba tekstów: {len(corpus)}\")\n",
    "    print(f\"nDCG: {ndcg:.3f}\")\n",
    "    print(f\"Wynik punktowy: {points}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138aec2c",
   "metadata": {
    "id": "138aec2c"
   },
   "source": [
    "Podczas sprawdzania model zostanie zapisany jako `your_model.pkl` i oceniony na zbiorze testowym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85df2162",
   "metadata": {
    "id": "85df2162",
    "tags": []
   },
   "outputs": [],
   "source": [
    "######################### NIE ZMIENIAJ TEJ KOMÓRKI ##########################\n",
    "\n",
    "if FINAL_EVALUATION_MODE:\n",
    "    import cloudpickle\n",
    "\n",
    "    OUTPUT_PATH = \"file_output\"\n",
    "    FUNCTION_FILENAME = \"your_model.pkl\"\n",
    "    FUNCTION_OUTPUT_PATH = os.path.join(OUTPUT_PATH, FUNCTION_FILENAME)\n",
    "\n",
    "    if not os.path.exists(OUTPUT_PATH):\n",
    "        os.makedirs(OUTPUT_PATH)\n",
    "\n",
    "    with open(FUNCTION_OUTPUT_PATH, \"wb\") as f:\n",
    "        cloudpickle.dump(Embedder, f)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "OAI Kernel (Python 3.11)",
   "language": "python",
   "name": "micromamba-env-olimpiadaai-olimpiadaai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
